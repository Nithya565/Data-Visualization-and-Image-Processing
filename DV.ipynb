{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:tf] *",
      "language": "python",
      "name": "conda-env-tf-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "DV.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VtibiJR8rXx"
      },
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import os.path\n",
        "from os import mkdir\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "import shutil\n",
        "\n",
        "\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn.utils.weight_norm import WeightNorm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0nI_ks9duko"
      },
      "source": [
        "def check_dir(path):\r\n",
        "    '''\r\n",
        "    Create directory if it does not exist.\r\n",
        "        path:           Path of directory.\r\n",
        "    '''\r\n",
        "    if not os.path.exists(path):\r\n",
        "        os.makedirs(path)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFtTOnnVekK1"
      },
      "source": [
        "def log(log_file_path, string):\r\n",
        "    '''\r\n",
        "    Write one line of log into screen and file.\r\n",
        "        log_file_path: Path of log file.\r\n",
        "        string:        String to write in log file.\r\n",
        "    '''\r\n",
        "    with open(log_file_path, 'a+') as f:\r\n",
        "        f.write(string + '\\n')\r\n",
        "        f.flush()\r\n",
        "    print(string)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSAsgjDBehiQ"
      },
      "source": [
        "class Timer():\r\n",
        "    def __init__(self):\r\n",
        "        self.o = time.time()\r\n",
        "\r\n",
        "    def measure(self, p=1):\r\n",
        "        x = (time.time() - self.o) / float(p)\r\n",
        "        x = int(x)\r\n",
        "        if x >= 3600:\r\n",
        "            return '{:.1f}h'.format(x / 3600)\r\n",
        "        if x >= 60:\r\n",
        "            return '{}m'.format(round(x / 60))\r\n",
        "        return '{}s'.format(x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAHY_RyneXMS"
      },
      "source": [
        "def top1accuracy(output, target):\r\n",
        "    _, pred = output.max(dim=1)\r\n",
        "    pred = pred.view(-1)\r\n",
        "    target = target.view(-1)\r\n",
        "    accuracy = 100 * pred.eq(target).float().mean()\r\n",
        "    return accuracy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I7XHxj0eRJC"
      },
      "source": [
        "def one_hot(indices, depth):\r\n",
        "\r\n",
        "    encoded_indicies = torch.zeros(indices.size() + torch.Size([depth])).cuda()\r\n",
        "    index = indices.view(indices.size() + torch.Size([1]))\r\n",
        "    encoded_indicies = encoded_indicies.scatter_(1, index, 1)\r\n",
        "\r\n",
        "    return encoded_indicies"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_htX3lWePMY"
      },
      "source": [
        "def top1accuracy_all(output, target,nKbase):\r\n",
        "\r\n",
        "    preds_data = output.data.cpu()\r\n",
        "    labels_test_data = target.data.cpu()\r\n",
        "    nKbase = nKbase.data.cpu()\r\n",
        "\r\n",
        "    base_ids = torch.nonzero(labels_test_data < nKbase).view(-1)\r\n",
        "    novel_ids = torch.nonzero(labels_test_data >= nKbase).view(-1)\r\n",
        "\r\n",
        "    preds_base = preds_data[base_ids, :]\r\n",
        "    preds_novel = preds_data[novel_ids, :]\r\n",
        "\r\n",
        "    AccuracyBoth = top1accuracy(preds_data, labels_test_data)\r\n",
        "    AccuracyBase = top1accuracy(preds_base[:, :nKbase], labels_test_data[base_ids])\r\n",
        "    AccuracyNovel = top1accuracy(preds_novel[:, nKbase:], (labels_test_data[novel_ids] - nKbase))\r\n",
        "\r\n",
        "    return AccuracyBoth, AccuracyBase, AccuracyNovel"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptl25Y5neJWg"
      },
      "source": [
        "def ensure_path(path):\r\n",
        "    if os.path.exists(path):\r\n",
        "        if input('{} exists, remove? ([y]/n)'.format(path)) != 'n':\r\n",
        "            shutil.rmtree(path)\r\n",
        "            os.mkdir(path)\r\n",
        "    else:\r\n",
        "        os.mkdir(path)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2vkiSt9eFkt"
      },
      "source": [
        "def set_gpu(gpu):\r\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = gpu\r\n",
        "    print('using gpu {}'.format(gpu))\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlZJgnv8eBRx"
      },
      "source": [
        "def pick_vectors(dic, wnids, is_tensor=False):\r\n",
        "    o = next(iter(dic.values()))\r\n",
        "    dim = len(o)\r\n",
        "    ret = []\r\n",
        "    for wnid in wnids:\r\n",
        "        v = dic.get(wnid)\r\n",
        "        if v is None:\r\n",
        "            if not is_tensor:\r\n",
        "                v = [0] * dim\r\n",
        "            else:\r\n",
        "                v = torch.zeros(dim)\r\n",
        "        ret.append(v)\r\n",
        "    if not is_tensor:\r\n",
        "        return torch.FloatTensor(ret)\r\n",
        "    else:\r\n",
        "        return torch.stack(ret)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGrKZJKzd9HD"
      },
      "source": [
        "def l2_loss(a, b):\r\n",
        "    return ((a - b)**2).sum() / (len(a) * 2)\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mKGRZucd6lG"
      },
      "source": [
        "def cosine_similarity(a,b,mask):\r\n",
        "    return torch.mm(a[mask],b[:,mask])\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxMiPgmLd3pH"
      },
      "source": [
        "def normt_spm(mx, method='in'):\r\n",
        "    if method == 'in':\r\n",
        "        mx = mx.transpose()\r\n",
        "        rowsum = np.array(mx.sum(1))\r\n",
        "        r_inv = np.power(rowsum, -1).flatten()\r\n",
        "        r_inv[np.isinf(r_inv)] = 0.\r\n",
        "        r_mat_inv = sp.diags(r_inv)\r\n",
        "        mx = r_mat_inv.dot(mx)\r\n",
        "        return mx\r\n",
        "\r\n",
        "    if method == 'sym':\r\n",
        "        rowsum = np.array(mx.sum(1))\r\n",
        "        r_inv = np.power(rowsum, -0.5).flatten()\r\n",
        "        r_inv[np.isinf(r_inv)] = 0.\r\n",
        "        r_mat_inv = sp.diags(r_inv)\r\n",
        "        mx = mx.dot(r_mat_inv).transpose().dot(r_mat_inv)\r\n",
        "        return mx"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cna4qo8TdzcL"
      },
      "source": [
        "def spm_to_tensor(sparse_mx):\r\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\r\n",
        "    indices = torch.from_numpy(np.vstack(\r\n",
        "            (sparse_mx.row, sparse_mx.col))).long()\r\n",
        "    values = torch.from_numpy(sparse_mx.data)\r\n",
        "    shape = torch.Size(sparse_mx.shape)\r\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Lze77a8rYG"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO4Lexso8rYH",
        "outputId": "db7a5305-14c2-44b6-d4d6-b158ef1fdb1f"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45jnPrn0BOp_",
        "outputId": "a16ef3b1-28a5-4f2d-d854-f88f70fa8b1c"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBltyGUFBYWt",
        "outputId": "d1a7ce45-b756-4012-bba2-c34e85756db8"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y02iIeQBpFMn"
      },
      "source": [
        "_MINI_IMAGENET_DATASET_DIR = \"/content/drive/My Drive/miniImagenet\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzNpK2tv9YgW",
        "outputId": "9df966fb-d351-4c24-9852-7ec0e17b872d"
      },
      "source": [
        "!pip install torchnet"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchnet in /usr/local/lib/python3.6/dist-packages (0.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchnet) (1.15.0)\n",
            "Requirement already satisfied: visdom in /usr/local/lib/python3.6/dist-packages (from torchnet) (0.1.8.9)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchnet) (1.7.0+cu101)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (20.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (1.4.1)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (1.28)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (1.19.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (7.0.0)\n",
            "Requirement already satisfied: torchfile in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (0.1.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (0.57.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (5.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom->torchnet) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchnet) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchnet) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchnet) (0.8)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.6/dist-packages (from jsonpatch->visdom->torchnet) (2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->torchnet) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->torchnet) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->torchnet) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom->torchnet) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vs882Or8rYI"
      },
      "source": [
        "import torchnet as tnt\n",
        "from PIL import Image\n",
        "import math\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awLzFyvq8rYI"
      },
      "source": [
        "def load_data(file):\n",
        "    try:\n",
        "        with open(file, 'rb') as fo:\n",
        "            data = pickle.load(fo)\n",
        "        return data\n",
        "    except:\n",
        "        with open(file, 'rb') as f:\n",
        "            u = pickle._Unpickler(f)\n",
        "            u.encoding = 'latin1'\n",
        "            data = u.load()\n",
        "        \n",
        "        return data"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTTWJuKde8Ag"
      },
      "source": [
        "def buildLabelIndex(labels):\r\n",
        "    label2inds = {}\r\n",
        "    for idx, label in enumerate(labels):\r\n",
        "        if label not in label2inds:\r\n",
        "            label2inds[label] = []\r\n",
        "        label2inds[label].append(idx)\r\n",
        "\r\n",
        "    return label2inds"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kIAC_5W8rYJ"
      },
      "source": [
        "class MiniImageNet(data.Dataset):\n",
        "\n",
        "    def __init__(self, phase='train', do_not_use_random_transf=False):\n",
        "\n",
        "        self.base_folder = 'miniImagenet'\n",
        "        assert (phase == 'train' or phase == 'val' or phase == 'test')\n",
        "        self.phase = phase\n",
        "        self.name = 'MiniImageNet_' + phase\n",
        "\n",
        "        print('Loading mini ImageNet dataset - phase {0}'.format(phase))\n",
        "\n",
        "        file_train_categories_train_phase = os.path.join(\n",
        "            _MINI_IMAGENET_DATASET_DIR,\n",
        "            'miniImageNet_category_split_train_phase_train.pickle')\n",
        "\n",
        "        file_train_categories_val_phase = os.path.join(\n",
        "            _MINI_IMAGENET_DATASET_DIR,\n",
        "            'miniImageNet_category_split_train_phase_val.pickle')\n",
        "\n",
        "        file_train_categories_test_phase = os.path.join(\n",
        "            _MINI_IMAGENET_DATASET_DIR,\n",
        "            'miniImageNet_category_split_train_phase_test.pickle')\n",
        "\n",
        "        file_val_categories_val_phase = os.path.join(\n",
        "            _MINI_IMAGENET_DATASET_DIR,\n",
        "            'miniImageNet_category_split_val.pickle')\n",
        "\n",
        "        file_test_categories_test_phase = os.path.join(\n",
        "            _MINI_IMAGENET_DATASET_DIR,\n",
        "            'miniImageNet_category_split_test.pickle')\n",
        "\n",
        "        if self.phase == 'train':\n",
        "            # During training phase we only load the training phase images of the training categories (aka base categories).\n",
        "            data_train = load_data(file_train_categories_train_phase)\n",
        "            self.data = data_train['data']\n",
        "            self.labels = data_train['labels']\n",
        "\n",
        "            self.label2ind = buildLabelIndex(self.labels)\n",
        "            self.labelIds = sorted(self.label2ind.keys())\n",
        "\n",
        "            self.num_cats = len(self.labelIds)\n",
        "\n",
        "            self.labelIds_base = self.labelIds\n",
        "\n",
        "            self.num_cats_base = len(self.labelIds_base)\n",
        "\n",
        "        elif self.phase == 'val' or self.phase == 'test':\n",
        "            if self.phase == 'test':\n",
        "                # load data that will be used for evaluating the recognition accuracy of the base categories.\n",
        "                data_base = load_data(file_train_categories_test_phase)\n",
        "                # load data that will be use for evaluating the few-shot recogniton accuracy on the novel categories.\n",
        "                data_novel = load_data(file_test_categories_test_phase)\n",
        "            elif self.phase == 'val':\n",
        "                # load data that will be used for evaluating the recognition accuracy of the base categories.\n",
        "                data_base = load_data(file_train_categories_val_phase)\n",
        "                # load data that will be use for evaluating the few-shot recogniton accuracy on the novel categories.\n",
        "                data_novel = load_data(file_val_categories_val_phase)\n",
        "\n",
        "            self.data = np.concatenate(\n",
        "                [data_base['data'], data_novel['data']], axis=0)\n",
        "            self.labels = data_base['labels'] + data_novel['labels']\n",
        "\n",
        "\n",
        "            self.label2ind = buildLabelIndex(self.labels)\n",
        "            self.labelIds = sorted(self.label2ind.keys())\n",
        "            self.num_cats = len(self.labelIds)\n",
        "\n",
        "            self.labelIds_base = buildLabelIndex(data_base['labels']).keys()\n",
        "            self.labelIds_novel = buildLabelIndex(data_novel['labels']).keys()\n",
        "\n",
        "\n",
        "            self.num_cats_base = len(self.labelIds_base)\n",
        "            self.num_cats_novel = len(self.labelIds_novel)\n",
        "\n",
        "            intersection = set(self.labelIds_base) & set(self.labelIds_novel)\n",
        "\n",
        "\n",
        "            assert (len(intersection) == 0)\n",
        "        else:\n",
        "            raise ValueError('Not valid phase {0}'.format(self.phase))\n",
        "\n",
        "        mean_pix = [x / 255.0 for x in [120.39586422, 115.59361427, 104.54012653]]\n",
        "        std_pix = [x / 255.0 for x in [70.68188272, 68.27635443, 72.54505529]]\n",
        "        normalize = transforms.Normalize(mean=mean_pix, std=std_pix)\n",
        "\n",
        "        if (self.phase == 'test' or self.phase == 'val') or (do_not_use_random_transf == True):\n",
        "            self.transform = transforms.Compose([\n",
        "                lambda x: np.asarray(x),\n",
        "                transforms.ToTensor(),\n",
        "                normalize\n",
        "            ])\n",
        "        else:\n",
        "\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.RandomCrop(84, padding=8),\n",
        "                # transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                lambda x: np.asarray(x),\n",
        "                transforms.ToTensor(),\n",
        "                normalize\n",
        "            ])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, label = self.data[index], self.labels[index]\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE9cO1aQe3fT"
      },
      "source": [
        "class FewShotDataloader():\r\n",
        "    def __init__(self,\r\n",
        "                 dataset,\r\n",
        "                 nKnovel=5, # number of novel categories.\r\n",
        "                 nKbase=-1, # number of base categories.\r\n",
        "                 nExemplars=1, # number of training examples per novel category.\r\n",
        "                 nTestNovel=15*5, # number of test examples for all the novel categories.\r\n",
        "                 nTestBase=15*5, # number of test examples for all the base categories.\r\n",
        "                 batch_size=1, # number of training episodes per batch.\r\n",
        "                 num_workers=4,\r\n",
        "                 epoch_size=2000, # number of batches per epoch.\r\n",
        "                 ):\r\n",
        "\r\n",
        "        self.dataset = dataset\r\n",
        "\r\n",
        "        self.phase = self.dataset.phase\r\n",
        "\r\n",
        "        max_possible_nKnovel = (self.dataset.num_cats_base if self.phase=='train'\r\n",
        "                                else self.dataset.num_cats_novel)\r\n",
        "        assert(nKnovel >= 0 and nKnovel < max_possible_nKnovel)\r\n",
        "\r\n",
        "        self.nKnovel = nKnovel\r\n",
        "\r\n",
        "        max_possible_nKbase = self.dataset.num_cats_base\r\n",
        "        nKbase = nKbase if nKbase >= 0 else max_possible_nKbase\r\n",
        "        if self.phase=='train' and nKbase > 0:\r\n",
        "            nKbase -= self.nKnovel\r\n",
        "            max_possible_nKbase -= self.nKnovel\r\n",
        "\r\n",
        "        assert(nKbase >= 0 and nKbase <= max_possible_nKbase)\r\n",
        "        self.nKbase = nKbase\r\n",
        "\r\n",
        "        self.nExemplars = nExemplars\r\n",
        "        self.nTestNovel = nTestNovel\r\n",
        "        self.nTestBase = nTestBase\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.epoch_size = epoch_size\r\n",
        "        self.num_workers = num_workers\r\n",
        "        self.is_eval_mode = (self.phase=='test') or (self.phase=='val')\r\n",
        "\r\n",
        "    def sampleImageIdsFrom(self, cat_id, sample_size=1):\r\n",
        "        \"\"\"\r\n",
        "        Samples `sample_size` number of unique image ids picked from the\r\n",
        "        category `cat_id` (i.e., self.dataset.label2ind[cat_id]).\r\n",
        "        Args:\r\n",
        "            cat_id: a scalar with the id of the category from which images will\r\n",
        "                be sampled.\r\n",
        "            sample_size: number of images that will be sampled.\r\n",
        "        Returns:\r\n",
        "            image_ids: a list of length `sample_size` with unique image ids.\r\n",
        "        \"\"\"\r\n",
        "        assert(cat_id in self.dataset.label2ind)\r\n",
        "        assert(len(self.dataset.label2ind[cat_id]) >= sample_size)\r\n",
        "        # Note: random.sample samples elements without replacement.\r\n",
        "        return random.sample(self.dataset.label2ind[cat_id], sample_size)\r\n",
        "\r\n",
        "    def sampleCategories(self, cat_set, sample_size=1):\r\n",
        "        \"\"\"\r\n",
        "        Samples `sample_size` number of unique categories picked from the\r\n",
        "        `cat_set` set of categories. `cat_set` can be either 'base' or 'novel'.\r\n",
        "        Args:\r\n",
        "            cat_set: string that specifies the set of categories from which\r\n",
        "                categories will be sampled.\r\n",
        "            sample_size: number of categories that will be sampled.\r\n",
        "        Returns:\r\n",
        "            cat_ids: a list of length `sample_size` with unique category ids.\r\n",
        "        \"\"\"\r\n",
        "        if cat_set=='base':\r\n",
        "            labelIds = self.dataset.labelIds_base\r\n",
        "        elif cat_set=='novel':\r\n",
        "            labelIds = self.dataset.labelIds_novel\r\n",
        "        else:\r\n",
        "            raise ValueError('Not recognized category set {}'.format(cat_set))\r\n",
        "\r\n",
        "        assert(len(labelIds) >= sample_size)\r\n",
        "        # return sample_size unique categories chosen from labelIds set of\r\n",
        "        # categories (that can be either self.labelIds_base or self.labelIds_novel)\r\n",
        "        # Note: random.sample samples elements without replacement.\r\n",
        "\r\n",
        "        return random.sample(labelIds, sample_size)\r\n",
        "\r\n",
        "    def sample_base_and_novel_categories(self, nKbase, nKnovel):\r\n",
        "        \"\"\"\r\n",
        "        Samples `nKbase` number of base categories and `nKnovel` number of novel\r\n",
        "        categories.\r\n",
        "        Args:\r\n",
        "            nKbase: number of base categories\r\n",
        "            nKnovel: number of novel categories\r\n",
        "        Returns:\r\n",
        "            Kbase: a list of length 'nKbase' with the ids of the sampled base\r\n",
        "                categories.\r\n",
        "            Knovel: a list of lenght 'nKnovel' with the ids of the sampled novel\r\n",
        "                categories.\r\n",
        "        \"\"\"\r\n",
        "        if self.is_eval_mode:\r\n",
        "            assert(nKnovel <= self.dataset.num_cats_novel)\r\n",
        "            # sample from the set of base categories 'nKbase' number of base\r\n",
        "            # categories.\r\n",
        "            Kbase = sorted(self.sampleCategories('base', nKbase))\r\n",
        "            # sample from the set of novel categories 'nKnovel' number of novel\r\n",
        "            # categories.\r\n",
        "            Knovel = sorted(self.sampleCategories('novel', nKnovel))\r\n",
        "        else:\r\n",
        "            # sample from the set of base categories 'nKnovel' + 'nKbase' number\r\n",
        "            # of categories.\r\n",
        "            cats_ids = self.sampleCategories('base', nKnovel+nKbase)\r\n",
        "            assert(len(cats_ids) == (nKnovel+nKbase))\r\n",
        "            # Randomly pick 'nKnovel' number of fake novel categories and keep\r\n",
        "            # the rest as base categories.\r\n",
        "            random.shuffle(cats_ids)\r\n",
        "            Knovel = sorted(cats_ids[:nKnovel])\r\n",
        "            Kbase = sorted(cats_ids[nKnovel:])\r\n",
        "\r\n",
        "        return Kbase, Knovel\r\n",
        "\r\n",
        "    def sample_test_examples_for_base_categories(self, Kbase, nTestBase):\r\n",
        "        \"\"\"\r\n",
        "        Sample `nTestBase` number of images from the `Kbase` categories.\r\n",
        "        Args:\r\n",
        "            Kbase: a list of length `nKbase` with the ids of the categories from\r\n",
        "                where the images will be sampled.\r\n",
        "            nTestBase: the total number of images that will be sampled.\r\n",
        "        Returns:\r\n",
        "            Tbase: a list of length `nTestBase` with 2-element tuples. The 1st\r\n",
        "                element of each tuple is the image id that was sampled and the\r\n",
        "                2nd elemend is its category label (which is in the range\r\n",
        "                [0, len(Kbase)-1]).\r\n",
        "        \"\"\"\r\n",
        "        Tbase = []\r\n",
        "        if len(Kbase) > 0:\r\n",
        "            # Sample for each base category a number images such that the total\r\n",
        "            # number sampled images of all categories to be equal to `nTestBase`.\r\n",
        "            KbaseIndices = np.random.choice(\r\n",
        "                np.arange(len(Kbase)), size=nTestBase, replace=True)\r\n",
        "            KbaseIndices, NumImagesPerCategory = np.unique(\r\n",
        "                KbaseIndices, return_counts=True)\r\n",
        "\r\n",
        "            for Kbase_idx, NumImages in zip(KbaseIndices, NumImagesPerCategory):\r\n",
        "                imd_ids = self.sampleImageIdsFrom(\r\n",
        "                    Kbase[Kbase_idx], sample_size=NumImages)\r\n",
        "                Tbase += [(img_id, Kbase_idx) for img_id in imd_ids]\r\n",
        "\r\n",
        "        assert(len(Tbase) == nTestBase)\r\n",
        "\r\n",
        "        return Tbase\r\n",
        "\r\n",
        "    def sample_train_and_test_examples_for_novel_categories(\r\n",
        "            self, Knovel, nTestNovel, nExemplars, nKbase):\r\n",
        "        \"\"\"Samples train and test examples of the novel categories.\r\n",
        "        Args:\r\n",
        "    \t    Knovel: a list with the ids of the novel categories.\r\n",
        "            nTestNovel: the total number of test images that will be sampled\r\n",
        "                from all the novel categories.\r\n",
        "            nExemplars: the number of training examples per novel category that\r\n",
        "                will be sampled.\r\n",
        "            nKbase: the number of base categories. It is used as offset of the\r\n",
        "                category index of each sampled image.\r\n",
        "        Returns:\r\n",
        "            Tnovel: a list of length `nTestNovel` with 2-element tuples. The\r\n",
        "                1st element of each tuple is the image id that was sampled and\r\n",
        "                the 2nd element is its category label (which is in the range\r\n",
        "                [nKbase, nKbase + len(Knovel) - 1]).\r\n",
        "            Exemplars: a list of length len(Knovel) * nExemplars of 2-element\r\n",
        "                tuples. The 1st element of each tuple is the image id that was\r\n",
        "                sampled and the 2nd element is its category label (which is in\r\n",
        "                the ragne [nKbase, nKbase + len(Knovel) - 1]).\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        if len(Knovel) == 0:\r\n",
        "            return [], []\r\n",
        "\r\n",
        "        nKnovel = len(Knovel)\r\n",
        "        Tnovel = []\r\n",
        "        Exemplars = []\r\n",
        "        assert((nTestNovel % nKnovel) == 0)\r\n",
        "        nEvalExamplesPerClass = int(nTestNovel / nKnovel)\r\n",
        "\r\n",
        "        for Knovel_idx in range(len(Knovel)):\r\n",
        "            imd_ids = self.sampleImageIdsFrom(\r\n",
        "                Knovel[Knovel_idx],\r\n",
        "                sample_size=(nEvalExamplesPerClass + nExemplars))\r\n",
        "\r\n",
        "            imds_tnovel = imd_ids[:nEvalExamplesPerClass]\r\n",
        "            imds_ememplars = imd_ids[nEvalExamplesPerClass:]\r\n",
        "\r\n",
        "            Tnovel += [(img_id, nKbase+Knovel_idx) for img_id in imds_tnovel]\r\n",
        "            Exemplars += [(img_id, nKbase+Knovel_idx) for img_id in imds_ememplars]\r\n",
        "        assert(len(Tnovel) == nTestNovel)\r\n",
        "        assert(len(Exemplars) == len(Knovel) * nExemplars)\r\n",
        "        random.shuffle(Exemplars)\r\n",
        "\r\n",
        "        return Tnovel, Exemplars\r\n",
        "\r\n",
        "    def sample_episode(self):\r\n",
        "        \"\"\"Samples a training episode.\"\"\"\r\n",
        "        nKnovel = self.nKnovel\r\n",
        "        nKbase = self.nKbase\r\n",
        "        nTestNovel = self.nTestNovel\r\n",
        "        nTestBase = self.nTestBase\r\n",
        "        nExemplars = self.nExemplars\r\n",
        "\r\n",
        "        Kbase, Knovel = self.sample_base_and_novel_categories(nKbase, nKnovel)\r\n",
        "        Tbase = self.sample_test_examples_for_base_categories(Kbase, nTestBase)\r\n",
        "        Tnovel, Exemplars = self.sample_train_and_test_examples_for_novel_categories(\r\n",
        "            Knovel, nTestNovel, nExemplars, nKbase)\r\n",
        "\r\n",
        "        # concatenate the base and novel category examples.\r\n",
        "        Test = Tbase + Tnovel\r\n",
        "        random.shuffle(Test)\r\n",
        "        Kall = Kbase + Knovel\r\n",
        "\r\n",
        "        return Exemplars, Test, Kall, nKbase\r\n",
        "\r\n",
        "    def createExamplesTensorData(self, examples):\r\n",
        "        \"\"\"\r\n",
        "        Creates the examples image and label tensor data.\r\n",
        "        Args:\r\n",
        "            examples: a list of 2-element tuples, each representing a\r\n",
        "                train or test example. The 1st element of each tuple\r\n",
        "                is the image id of the example and 2nd element is the\r\n",
        "                category label of the example, which is in the range\r\n",
        "                [0, nK - 1], where nK is the total number of categories\r\n",
        "                (both novel and base).\r\n",
        "        Returns:\r\n",
        "            images: a tensor of shape [nExamples, Height, Width, 3] with the\r\n",
        "                example images, where nExamples is the number of examples\r\n",
        "                (i.e., nExamples = len(examples)).\r\n",
        "            labels: a tensor of shape [nExamples] with the category label\r\n",
        "                of each example.\r\n",
        "        \"\"\"\r\n",
        "        images = torch.stack(\r\n",
        "            [self.dataset[img_idx][0] for img_idx, _ in examples], dim=0)\r\n",
        "        labels = torch.LongTensor([label for _, label in examples])\r\n",
        "        return images, labels\r\n",
        "\r\n",
        "    def get_iterator(self, epoch=0):\r\n",
        "        rand_seed = epoch\r\n",
        "        random.seed(rand_seed)\r\n",
        "        np.random.seed(rand_seed)\r\n",
        "        def load_function(iter_idx):\r\n",
        "            Exemplars, Test, Kall, nKbase = self.sample_episode()\r\n",
        "            Xt, Yt = self.createExamplesTensorData(Test)\r\n",
        "            Kall = torch.LongTensor(Kall)\r\n",
        "            if len(Exemplars) > 0:\r\n",
        "                Xe, Ye = self.createExamplesTensorData(Exemplars)\r\n",
        "                return Xe, Ye, Xt, Yt, Kall, nKbase\r\n",
        "            else:\r\n",
        "                return Xt, Yt, Kall, nKbase\r\n",
        "\r\n",
        "        tnt_dataset = tnt.dataset.ListDataset(\r\n",
        "            elem_list=range(self.epoch_size), load=load_function)\r\n",
        "        data_loader = tnt_dataset.parallel(\r\n",
        "            batch_size=self.batch_size,\r\n",
        "            num_workers=(0 if self.is_eval_mode else self.num_workers),\r\n",
        "            shuffle=(False if self.is_eval_mode else True))\r\n",
        "\r\n",
        "        return data_loader\r\n",
        "\r\n",
        "    def __call__(self, epoch=0):\r\n",
        "        return self.get_iterator(epoch)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return int(self.epoch_size / self.batch_size)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njHGZ2xC8rYV"
      },
      "source": [
        "# Convolution model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAIOZ-oL8rYg"
      },
      "source": [
        "# Convolution module\n",
        "class ConvBlock(nn.Module):\n",
        "    # Initialize the instance object\n",
        "    def __init__(self, in_planes, out_planes, userelu=True):\n",
        "        # Initialize the properties inherited from the parent class\n",
        "        super(ConvBlock, self).__init__()\n",
        "        # Time series container, quickly build neural network\n",
        "        self.layers = nn.Sequential()\n",
        "        # Convolutional layer, the size of the convolution kernel is 3*3, the step size is 1, the padding is 1\n",
        "        self.layers.add_module('Conv', nn.Conv2d(in_planes, out_planes,\n",
        "            kernel_size=3, stride=1, padding=1, bias=False))\n",
        "\n",
        "        # batchnorm Floor\n",
        "        self.layers.add_module('BatchNorm', nn.BatchNorm2d(out_planes))\n",
        "\n",
        "        # Relu Floor\n",
        "        if userelu:\n",
        "            self.layers.add_module('ReLU', nn.ReLU(inplace=True))\n",
        "\n",
        "        # Maximum pooling layer, size 2*2\n",
        "        self.layers.add_module(\n",
        "            'MaxPool', nn.MaxPool2d(kernel_size=2, stride=2, padding=0))\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "    # Get output\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UuT90qAfrA8"
      },
      "source": [
        "# Convolutional network\r\n",
        "class ConvNet(nn.Module):\r\n",
        "    # Initialize the instance object\r\n",
        "    def __init__(self, in_planes, out_planes, num_stages,userelu=False):\r\n",
        "\r\n",
        "        # Initialize the properties inherited from the parent class\r\n",
        "        super(ConvNet, self).__init__()\r\n",
        "\r\n",
        "        # Get the size of the input channel\r\n",
        "        self.in_planes  = in_planes\r\n",
        "\r\n",
        "        # Get the size of the output channel\r\n",
        "        self.out_planes = out_planes\r\n",
        "\r\n",
        "        # Get the number of network layers\r\n",
        "        self.num_stages = num_stages\r\n",
        "\r\n",
        "        # Get the number of output channels per layer\r\n",
        "        if type(self.out_planes) == int:\r\n",
        "            self.out_planes = [self.out_planes for i in range(self.num_stages)]\r\n",
        "        assert(type(self.out_planes)==list and len(self.out_planes)==self.num_stages)\r\n",
        "\r\n",
        "        num_planes = [self.in_planes,] + self.out_planes\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        conv_blocks = []\r\n",
        "        # Define 4-layer convolutional network\r\n",
        "        for i in range(self.num_stages):\r\n",
        "            if i == (self.num_stages-1):\r\n",
        "                conv_blocks.append(\r\n",
        "                    ConvBlock(num_planes[i], num_planes[i+1], userelu=userelu))\r\n",
        "            else:\r\n",
        "                conv_blocks.append(\r\n",
        "                    ConvBlock(num_planes[i], num_planes[i+1]))\r\n",
        "        # Building a convolutional network\r\n",
        "        self.conv_blocks = nn.Sequential(*conv_blocks)\r\n",
        "\r\n",
        "        # Initialize convolutional network parameters\r\n",
        "        for m in self.modules():\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\r\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                m.weight.data.fill_(1)\r\n",
        "                m.bias.data.zero_()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        # Forward propagation\r\n",
        "        out = self.conv_blocks(x)\r\n",
        "        # Convert output to 1 dimension\r\n",
        "        out = out.view(out.size(0),-1)\r\n",
        "        # Get output\r\n",
        "        return out\r\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inPFbbGWfrhU"
      },
      "source": [
        "# Build model\r\n",
        "# def create_model(opt):\r\n",
        "#     return ConvNet(opt)\r\n",
        "def Conv64():\r\n",
        "    return  ConvNet(in_planes=3,out_planes=[64,64,64,64],num_stages=4,userelu=False)\r\n",
        "\r\n",
        "def Conv128():\r\n",
        "    return  ConvNet(in_planes=3,out_planes=[64,64,128,128],num_stages=4,userelu=False)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abfLlPDS8rYg"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEb0GXDd8rYh"
      },
      "source": [
        "class LinearDiag(nn.Module):\n",
        "\n",
        "    def __init__(self, num_features, bias=False):\n",
        "\n",
        "        super(LinearDiag, self).__init__()\n",
        "\n",
        "        weight = torch.FloatTensor(num_features).fill_(1) # initialize to the identity transform\n",
        "        self.weight = nn.Parameter(weight, requires_grad=True)\n",
        "\n",
        "        if bias:\n",
        "            bias = torch.FloatTensor(num_features).fill_(0)\n",
        "            self.bias = nn.Parameter(bias, requires_grad=True)\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    def forward(self, X):\n",
        "        assert(X.dim()==2 and X.size(1)==self.weight.size(0))\n",
        "        out = X * self.weight.expand_as(X)\n",
        "        if self.bias is not None:\n",
        "            out = out + self.bias.expand_as(out)\n",
        "        return out"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4T4fWYZfYgH"
      },
      "source": [
        "class FeatExemplarAvgBlock(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, nFeat):\r\n",
        "        super(FeatExemplarAvgBlock, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, features_train, labels_train):\r\n",
        "        labels_train_transposed = labels_train.transpose(1,2)\r\n",
        "\r\n",
        "        weight_novel = torch.bmm(labels_train_transposed, features_train)\r\n",
        "        weight_novel = weight_novel.div(\r\n",
        "            labels_train_transposed.sum(dim=2, keepdim=True).expand_as(weight_novel))\r\n",
        "\r\n",
        "        return weight_novel"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOYrASI6fY1R"
      },
      "source": [
        "class KTN_Classifier(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self,nKall=64, nFeat=64 * 5 * 5, scale_cls=10.0):\r\n",
        "        super(KTN_Classifier, self).__init__()\r\n",
        "\r\n",
        "        self.favgblock = FeatExemplarAvgBlock(nFeat)\r\n",
        "\r\n",
        "        weight_base = torch.FloatTensor(nKall, nFeat).normal_(0.0, np.sqrt(2.0 / nFeat))\r\n",
        "        self.weight_base = nn.Parameter(weight_base, requires_grad=True)\r\n",
        "\r\n",
        "        self.bias = nn.Parameter(torch.FloatTensor(1).fill_(0), requires_grad=True)\r\n",
        "\r\n",
        "        self.scale_cls = nn.Parameter(torch.FloatTensor(1).fill_(scale_cls), requires_grad=True)\r\n",
        "\r\n",
        "\r\n",
        "    def get_classification_weights(\r\n",
        "            self, Kbase_ids, Knovel_ids, pred, features_support, labels_support):\r\n",
        "\r\n",
        "        batch_size, nKbase = Kbase_ids.size()\r\n",
        "\r\n",
        "        weight_base = self.weight_base[Kbase_ids.view(-1)]\r\n",
        "        weight_base = weight_base.view(batch_size, nKbase, -1)\r\n",
        "\r\n",
        "        if features_support is None or labels_support is None:\r\n",
        "\r\n",
        "            return weight_base\r\n",
        "\r\n",
        "        else:\r\n",
        "\r\n",
        "            _, num_train_examples, num_channels = features_support.size()\r\n",
        "\r\n",
        "            nKnovel = labels_support.size(2)\r\n",
        "\r\n",
        "            features_support = F.normalize(\r\n",
        "                    features_support, p=2, dim=features_support.dim() - 1, eps=1e-12)\r\n",
        "\r\n",
        "            weight_novel = self.favgblock(features_support, labels_support)\r\n",
        "            weight_novel = weight_novel.view(batch_size, nKnovel, num_channels)\r\n",
        "\r\n",
        "            if pred is not None:\r\n",
        "\r\n",
        "                fcw = torch.stack(\r\n",
        "                    (pred['pred'][Knovel_ids[0][0]], pred['pred'][Knovel_ids[0][1]], pred['pred'][Knovel_ids[0][2]],\r\n",
        "                     pred['pred'][Knovel_ids[0][3]], pred['pred'][Knovel_ids[0][4]]), 0)\r\n",
        "                fcw = fcw.view(batch_size, nKnovel, num_channels)\r\n",
        "\r\n",
        "                coefficient = float(5 / labels_support.size()[1])\r\n",
        "                weight_novel = weight_novel + fcw * coefficient\r\n",
        "\r\n",
        "            weight_both = torch.cat([weight_base, weight_novel], dim=1)\r\n",
        "\r\n",
        "            return weight_both\r\n",
        "\r\n",
        "    def apply_classification_weights(self, features, cls_weights):\r\n",
        "\r\n",
        "        features = F.normalize(\r\n",
        "                features, p=2, dim=features.dim() - 1, eps=1e-12)\r\n",
        "        cls_weights = F.normalize(\r\n",
        "                cls_weights, p=2, dim=cls_weights.dim() - 1, eps=1e-12)\r\n",
        "\r\n",
        "        cls_scores = self.scale_cls * torch.baddbmm(1.0,self.bias.view(1, 1, 1), 1.0, features,\r\n",
        "        cls_weights.transpose(1, 2))\r\n",
        "        return cls_scores\r\n",
        "\r\n",
        "    def forward(self, features_query=None, Kbase_ids=None, Knovel_ids=None, pred_fcw = None, features_support=None, labels_support=None):\r\n",
        "\r\n",
        "        cls_weights = self.get_classification_weights(\r\n",
        "            Kbase_ids, Knovel_ids, pred_fcw, features_support, labels_support)\r\n",
        "\r\n",
        "        cls_scores = self.apply_classification_weights(\r\n",
        "            features_query, cls_weights)\r\n",
        "\r\n",
        "        return cls_scores"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cLFkvvL8rYh"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhsAUfka91fW",
        "outputId": "1af82409-60b7-4b3a-9297-735e726a8538"
      },
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eXYFG6D8rYi"
      },
      "source": [
        "parser = argparse.ArgumentParser()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8etgiPY8rYj",
        "outputId": "8315be35-a65f-49b6-805c-c03d98f1d412"
      },
      "source": [
        "#********************* Model/Dataset/Path Config *********************#\n",
        "parser.add_argument('--num_epoch', type=int, default=2,\n",
        "                    help='number of training epochs')\n",
        "parser.add_argument('--save_path', default='experiments')\n",
        "parser.add_argument('--network', type=str, default='Conv128',\n",
        "                    help='choose which embedding network (Conv64/128) to use')\n",
        "parser.add_argument('--dataset', type=str, default='miniImageNet',\n",
        "                    help='choose which classification head to use. miniImageNet, tieredImageNet')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--dataset'], dest='dataset', nargs=None, const=None, default='miniImageNet', type=<class 'str'>, choices=None, help='choose which classification head to use. miniImageNet, tieredImageNet', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPN_GsE-8rYm",
        "outputId": "ceea2f3f-99cb-4121-dcef-2a498466e063"
      },
      "source": [
        "#********************* Training Config *********************#\n",
        "parser.add_argument('--train_nKnovel', type=int, default=0, help='number of novel categories during 1-stage_base training phase')\n",
        "parser.add_argument('--train_nKbase', type=int, default=64, help='number of base categories during 1-stage_base training phase')\n",
        "parser.add_argument('--train_nExemplars', type=int, default=0, help='number of support examples per novel category')\n",
        "parser.add_argument('--train_nTestNovel', type=int, default=0, help='number of query examples for all the novel category')\n",
        "parser.add_argument('--train_nTestBase', type=int, default=32, help='number of test examples for all the base category')\n",
        "parser.add_argument('--train_batch_size', type=int, default=8, help='number of episodes per batch')\n",
        "parser.add_argument('--train_epoch_size', type=int, default=8*1000, help='number of episodes per epoch')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--train_epoch_size'], dest='train_epoch_size', nargs=None, const=None, default=8000, type=<class 'int'>, choices=None, help='number of episodes per epoch', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSXyO21G8rYn",
        "outputId": "66821b80-670f-4961-d330-0c6b8558a98f"
      },
      "source": [
        "#********************* Valing Config *********************#\n",
        "parser.add_argument('--val_nKnovel', type=int, default=5, help='number of novel categories during 1-stage_base valing phase')\n",
        "parser.add_argument('--val_nKbase', type=int, default=64, help='number of base categories during 1-stage_base valing phase')\n",
        "parser.add_argument('--val_nExemplars', type=int, default=1, help='number of support examples per novel category')\n",
        "parser.add_argument('--val_nTestNovel', type=int, default=15*5, help='number of query examples for all the novel category')\n",
        "parser.add_argument('--val_nTestBase', type=int, default=15*5, help='number of test examples for all the base category')\n",
        "parser.add_argument('--val_batch_size', type=int, default=1, help='number of episodes per batch')\n",
        "parser.add_argument('--val_epoch_size', type=int, default=2000, help='number of batchs per epoch')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--val_epoch_size'], dest='val_epoch_size', nargs=None, const=None, default=2000, type=<class 'int'>, choices=None, help='number of batchs per epoch', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsD4WPX_8rYo"
      },
      "source": [
        "params = parser.parse_args(\"\")\n",
        "# all_defaults = {}\n",
        "# for key in vars(params):\n",
        "#     all_defaults[key] = parser.get_default(key)\n",
        "# all_defaults"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tzxj8nd8rYp",
        "outputId": "7b78b061-1ecd-4586-b942-a47731c95cd7"
      },
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "print('using gpu:', '0')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using gpu: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZhOr-Ay8rYq"
      },
      "source": [
        "save_path1 = os.path.join(params.save_path)\n",
        "mkdir((save_path1))\n",
        "save_path2 = os.path.join(params.save_path, params.dataset)\n",
        "mkdir((save_path2))\n",
        "params.network = ['Conv64', 'Conv128']\n",
        "dir_1 = os.path.join(params.save_path, params.dataset,params.network[0])\n",
        "dir_2 = os.path.join(params.save_path, params.dataset,params.network[1])\n",
        "mkdir((dir_1))\n",
        "mkdir((dir_2))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUEFrmfe8rYq",
        "outputId": "5c718503-32c3-479f-f06b-0a3308a2b8ed"
      },
      "source": [
        "log_file_path_1 = os.path.join(dir_1, \"train_log.txt\")\n",
        "log(log_file_path_1, str(vars(params)))\n",
        "log_file_path_2 = os.path.join(dir_2, \"train_log.txt\")\n",
        "log(log_file_path_2, str(vars(params)))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'num_epoch': 2, 'save_path': 'experiments', 'network': ['Conv64', 'Conv128'], 'dataset': 'miniImageNet', 'train_nKnovel': 0, 'train_nKbase': 64, 'train_nExemplars': 0, 'train_nTestNovel': 0, 'train_nTestBase': 32, 'train_batch_size': 8, 'train_epoch_size': 8000, 'val_nKnovel': 5, 'val_nKbase': 64, 'val_nExemplars': 1, 'val_nTestNovel': 75, 'val_nTestBase': 75, 'val_batch_size': 1, 'val_epoch_size': 2000}\n",
            "{'num_epoch': 2, 'save_path': 'experiments', 'network': ['Conv64', 'Conv128'], 'dataset': 'miniImageNet', 'train_nKnovel': 0, 'train_nKbase': 64, 'train_nExemplars': 0, 'train_nTestNovel': 0, 'train_nTestBase': 32, 'train_batch_size': 8, 'train_epoch_size': 8000, 'val_nKnovel': 5, 'val_nKbase': 64, 'val_nExemplars': 1, 'val_nTestNovel': 75, 'val_nTestBase': 75, 'val_batch_size': 1, 'val_epoch_size': 2000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwIL6my88rYr",
        "outputId": "fa88768b-a937-4e2b-f7b7-234ce4858080"
      },
      "source": [
        "dataset_train = MiniImageNet(phase=\"train\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading mini ImageNet dataset - phase train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTDs7Ncf8rYs",
        "outputId": "a9bacf82-40f4-4dda-b4fc-da84eaee29fb"
      },
      "source": [
        "dataset_val = MiniImageNet(phase='val')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading mini ImageNet dataset - phase val\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky6qnk7W8rYt"
      },
      "source": [
        "dataloder_train = FewShotDataloader(\n",
        "    dataset=dataset_train,\n",
        "    nKnovel=params.train_nKnovel,\n",
        "    nKbase=params.train_nKbase,\n",
        "    nExemplars=params.train_nExemplars,\n",
        "    nTestNovel=params.train_nTestNovel,\n",
        "    nTestBase=params.train_nTestBase,\n",
        "    batch_size=params.train_batch_size,\n",
        "    num_workers= 4,\n",
        "    epoch_size=params.train_epoch_size,\n",
        ")"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvBoCUHZ8rYu"
      },
      "source": [
        "dataloder_val = FewShotDataloader(\n",
        "    dataset=dataset_val,\n",
        "    nKnovel=params.val_nKnovel,\n",
        "    nKbase=params.val_nKbase,\n",
        "    nExemplars=params.val_nExemplars,\n",
        "    nTestNovel=params.val_nTestNovel,\n",
        "    nTestBase=params.val_nTestBase,\n",
        "    batch_size=params.val_batch_size,\n",
        "    num_workers= 4,\n",
        "    epoch_size=params.val_epoch_size,\n",
        ")"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bpNZcDD8rYu"
      },
      "source": [
        "for par in range(2):\n",
        "    if params.network[par] == 'Conv64':\n",
        "        embedding_model = Conv64().cuda()\n",
        "        classifier = KTN_Classifier(nKall=64, nFeat=64 * 5 * 5, scale_cls=10).cuda()\n",
        "    elif params.network[par] == 'Conv128':\n",
        "        embedding_model = Conv128().cuda()\n",
        "        classifier = KTN_Classifier(nKall=64, nFeat=128 * 5 * 5, scale_cls=10).cuda()\n",
        "    else:  # the other backbones are coming soon!\n",
        "        raise ValueError('Unknown models')\n",
        "    \n",
        "    optimizer = torch.optim.SGD([{'params': embedding_model.parameters()},{'params': classifier.parameters()}],\n",
        "                            lr=0.1, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
        "    lambda_epoch = lambda e: 1.0 if e < 20 else (0.06 if e < 40 else 0.012 if e < 50 else (0.0024))\n",
        "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch, last_epoch=-1)\n",
        "\n",
        "    max_val_acc = 0.0\n",
        "    max_val_epoch = 0.0\n",
        "\n",
        "    timer = Timer()\n",
        "    Loss = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kvYAHPD8rYy",
        "outputId": "2fe979dd-1846-41ec-a60d-dd894d4f12c8"
      },
      "source": [
        "dir = [dir_1, dir_2]\n",
        "#save_path = [save_path_1, save_path_2]\n",
        "log_file_path = [log_file_path_1, log_file_path_2]\n",
        "for l in range(2):\n",
        "    for epoch in range(1, params.num_epoch + 1):\n",
        "\n",
        "        epoch_learning_rate = 0.1\n",
        "        for param_group in optimizer.param_groups:\n",
        "            epoch_learning_rate = param_group['lr']\n",
        "\n",
        "        log(log_file_path[l], 'Train Epoch: {}\\tLearning Rate: {:.4f}'.format(epoch, epoch_learning_rate))\n",
        "\n",
        "        _, _ = [x.train() for x in (embedding_model, classifier)]\n",
        "        train_accuracies = []\n",
        "        train_losses = []\n",
        "\n",
        "        for i, batch in enumerate(tqdm(dataloder_train(epoch)), 1):\n",
        "\n",
        "            data_train, labels_train = batch[0].cuda(), batch[1].view(params.train_batch_size*params.train_nTestBase).cuda()\n",
        "            all_Kids, nKbase = batch[2], batch[3].squeeze()[0]\n",
        "\n",
        "            emb_data_train = embedding_model(data_train.reshape([-1] + list(data_train.shape[-3:])))\n",
        "            emb_data_train = emb_data_train.reshape(params.train_batch_size,params.train_nTestBase,-1)\n",
        "\n",
        "            Kbase_ids =  Variable(all_Kids[:, :nKbase].contiguous(), requires_grad=False)\n",
        "\n",
        "            cls_scores = classifier(features_query=emb_data_train, Kbase_ids=Kbase_ids).view(params.train_batch_size*params.train_nTestBase,-1)\n",
        "\n",
        "            loss = Loss(cls_scores,labels_train)\n",
        "            acc= top1accuracy(cls_scores.data, labels_train.data)\n",
        "\n",
        "            train_accuracies.append(acc.item())\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "            if (i % 200 == 0):\n",
        "                train_acc_avg = np.mean(np.array(train_accuracies))\n",
        "                log(log_file_path[l], 'Train Epoch: {}\\tBatch: [{}/{}]\\tLoss: {:.4f}\\tAccuracyBase: {:.2f} % ({:.2f} %)'.format(\n",
        "                    epoch, i, len(dataloder_train), loss.item(), train_acc_avg, acc))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        _, _ = [x.eval() for x in (embedding_model, classifier)]\n",
        "\n",
        "        val_accuracies_both = []\n",
        "        val_accuracies_base = []\n",
        "        val_accuracies_novel = []\n",
        "\n",
        "        for i, batch in enumerate(tqdm(dataloder_val(epoch)), 1):\n",
        "\n",
        "            data_support, labels_support, data_query, labels_query, all_Kids, nKbase = [x.cuda() for x in batch]\n",
        "\n",
        "            labels_support_one_hot = one_hot(labels_support.reshape(-1)-64,5).unsqueeze(dim=0)\n",
        "\n",
        "            Kbase_ids = Variable(all_Kids[:, :nKbase].contiguous(), requires_grad=False)\n",
        "\n",
        "            emb_data_support = embedding_model(data_support.view([-1] + list(data_support.shape[-3:])))\n",
        "            emb_data_support = emb_data_support.view(params.val_batch_size,params.val_nKnovel,-1)\n",
        "\n",
        "            emb_data_query = embedding_model(data_query.view([-1] + list(data_query.shape[-3:])))\n",
        "            emb_data_query = emb_data_query.view(params.val_batch_size, params.val_nTestBase + params.val_nTestNovel, -1)\n",
        "\n",
        "            cls_scores = classifier(features_query=emb_data_query, Kbase_ids=Kbase_ids,\n",
        "                                     features_support=emb_data_support, labels_support=labels_support_one_hot).view(\n",
        "                                           params.val_batch_size*(params.val_nTestBase+params.val_nTestNovel),-1)\n",
        "\n",
        "            accuracyBoth, accuracyBase, accuracyNovel = top1accuracy_all(cls_scores, labels_query.view(-1), nKbase)\n",
        "\n",
        "            val_accuracies_both.append(accuracyBoth.item())\n",
        "            val_accuracies_base.append(accuracyBase.item())\n",
        "            val_accuracies_novel.append(accuracyNovel.item())\n",
        "\n",
        "        val_acc_both = np.mean(np.array(val_accuracies_both))\n",
        "        val_acc_both_ci95 = 1.96 * np.std(np.array(val_accuracies_both)) / np.sqrt(params.val_epoch_size)\n",
        "\n",
        "        val_acc_base = np.mean(np.array(val_accuracies_base))\n",
        "        val_acc_base_ci95 = 1.96 * np.std(np.array(val_accuracies_base)) / np.sqrt(params.val_epoch_size)\n",
        "\n",
        "        val_acc_novel = np.mean(np.array(val_accuracies_novel))\n",
        "        val_acc_novel_ci95 = 1.96 * np.std(np.array(val_accuracies_novel)) / np.sqrt(params.val_epoch_size)\n",
        "\n",
        "        if val_acc_novel > max_val_acc:\n",
        "            max_val_acc = val_acc_novel\n",
        "            max_val_epoch = epoch\n",
        "            torch.save({'embedding': embedding_model.state_dict(), 'classifier': classifier.state_dict()}, os.path.join(params.save_path, params.dataset,params.network[l], '1_stage_best_model.pth'))\n",
        "            log(log_file_path[l], 'Best model saving!!!')\n",
        "            log(log_file_path[l],\n",
        "                'Val_Epoch: [{}/{}]\\tAccuracyBoth: {:.2f} +- {:.2f} %\\tAccuracyBase: {:.2f} +- {:.2f} %\\tAccuracyNovel: {:.2f} +- {:.2f} %'.format(\n",
        "                epoch, params.num_epoch,\n",
        "                val_acc_both, val_acc_both_ci95,\n",
        "                val_acc_base, val_acc_base_ci95,\n",
        "                val_acc_novel, val_acc_novel_ci95))\n",
        "        else:\n",
        "            log(log_file_path[l],\n",
        "                'Val_Epoch: [{}/{}]\\tAccuracyBoth: {:.2f} +- {:.2f} %\\tAccuracyBase: {:.2f} +- {:.2f} %\\tAccuracyNovel: {:.2f} +- {:.2f} %'.format(\n",
        "                    epoch, params.num_epoch,\n",
        "                    val_acc_both, val_acc_both_ci95,\n",
        "                    val_acc_base, val_acc_base_ci95,\n",
        "                    val_acc_novel, val_acc_novel_ci95))\n",
        "\n",
        "        torch.save({'embedding': embedding_model.state_dict(), 'classifier': classifier.state_dict()},\n",
        "               os.path.join(params.save_path, params.dataset,params.network[l], '1_stage_last_model.pth'))\n",
        "\n",
        "        log(log_file_path[l], 'Elapsed Time: {}/{}\\n'.format(timer.measure(), timer.measure(epoch / float(params.num_epoch))))\n",
        "    log(log_file_path[l],\n",
        "        'Best model saving!!!\\tBest_Epoch: [{}/{}]\\tAccuracyNovel: {:.2f} %'.format(\n",
        "        max_val_epoch, params.num_epoch,max_val_acc))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tLearning Rate: 0.1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py:74: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py:74: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py:74: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py:74: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: This overload of baddbmm is deprecated:\n",
            "\tbaddbmm(Number beta, Tensor input, Number alpha, Tensor batch1, Tensor batch2, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tbaddbmm(Tensor input, Tensor batch1, Tensor batch2, *, Number beta, Number alpha, Tensor out) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            " 20%|        | 201/1000 [00:32<02:09,  6.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tBatch: [200/1000]\tLoss: 2.6947\tAccuracyBase: 22.56 % (27.34 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|      | 401/1000 [01:04<01:25,  7.00it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tBatch: [400/1000]\tLoss: 2.2559\tAccuracyBase: 30.16 % (42.97 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 601/1000 [01:35<00:58,  6.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tBatch: [600/1000]\tLoss: 1.9051\tAccuracyBase: 34.46 % (51.95 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|  | 801/1000 [02:07<00:31,  6.28it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tBatch: [800/1000]\tLoss: 1.7531\tAccuracyBase: 37.77 % (53.91 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [02:37<00:00,  8.01it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tBatch: [1000/1000]\tLoss: 1.8109\tAccuracyBase: 40.32 % (55.08 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [02:38<00:00,  6.33it/s]\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py:74: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n",
            "100%|| 2000/2000 [02:49<00:00, 11.83it/s]\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best model saving!!!\n",
            "Val_Epoch: [1/2]\tAccuracyBoth: 31.83 +- 0.22 %\tAccuracyBase: 47.03 +- 0.25 %\tAccuracyNovel: 44.99 +- 0.41 %\n",
            "Elapsed Time: 5m/11m\n",
            "\n",
            "Train Epoch: 2\tLearning Rate: 0.1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|        | 201/1000 [00:30<01:52,  7.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tBatch: [200/1000]\tLoss: 1.7675\tAccuracyBase: 53.32 % (50.78 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|      | 400/1000 [00:59<01:23,  7.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tBatch: [400/1000]\tLoss: 1.5667\tAccuracyBase: 54.44 % (63.67 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 601/1000 [01:28<00:55,  7.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tBatch: [600/1000]\tLoss: 1.5547\tAccuracyBase: 55.28 % (59.38 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|  | 801/1000 [01:56<00:30,  6.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tBatch: [800/1000]\tLoss: 1.4305\tAccuracyBase: 56.23 % (64.06 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [02:24<00:00,  8.14it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tBatch: [1000/1000]\tLoss: 1.5504\tAccuracyBase: 56.97 % (55.47 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [02:24<00:00,  6.90it/s]\n",
            "100%|| 2000/2000 [02:46<00:00, 12.04it/s]\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best model saving!!!\n",
            "Val_Epoch: [2/2]\tAccuracyBoth: 34.54 +- 0.21 %\tAccuracyBase: 49.89 +- 0.24 %\tAccuracyNovel: 46.72 +- 0.42 %\n",
            "Elapsed Time: 11m/11m\n",
            "\n",
            "Best model saving!!!\tBest_Epoch: [2/2]\tAccuracyNovel: 46.72 %\n",
            "Train Epoch: 1\tLearning Rate: 0.1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|        | 201/1000 [00:29<01:56,  6.84it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tBatch: [200/1000]\tLoss: 1.4498\tAccuracyBase: 60.74 % (61.33 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|      | 401/1000 [00:58<01:21,  7.35it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tBatch: [400/1000]\tLoss: 1.4234\tAccuracyBase: 61.35 % (63.67 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 601/1000 [01:27<00:58,  6.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tBatch: [600/1000]\tLoss: 1.2194\tAccuracyBase: 61.91 % (67.97 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|  | 801/1000 [01:55<00:28,  6.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tBatch: [800/1000]\tLoss: 1.4182\tAccuracyBase: 62.28 % (63.67 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [02:23<00:00,  8.20it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1\tBatch: [1000/1000]\tLoss: 1.3373\tAccuracyBase: 62.55 % (61.72 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [02:24<00:00,  6.94it/s]\n",
            "100%|| 2000/2000 [02:45<00:00, 12.05it/s]\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val_Epoch: [1/2]\tAccuracyBoth: 34.66 +- 0.21 %\tAccuracyBase: 51.40 +- 0.25 %\tAccuracyNovel: 46.69 +- 0.41 %\n",
            "Elapsed Time: 16m/32m\n",
            "\n",
            "Train Epoch: 2\tLearning Rate: 0.1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|        | 201/1000 [00:30<01:59,  6.69it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tBatch: [200/1000]\tLoss: 1.1358\tAccuracyBase: 64.21 % (72.27 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|      | 401/1000 [00:58<01:28,  6.74it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tBatch: [400/1000]\tLoss: 1.2477\tAccuracyBase: 64.70 % (65.62 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|    | 601/1000 [01:27<00:55,  7.13it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tBatch: [600/1000]\tLoss: 1.2944\tAccuracyBase: 64.91 % (64.84 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|  | 801/1000 [01:55<00:27,  7.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tBatch: [800/1000]\tLoss: 1.2095\tAccuracyBase: 65.14 % (68.36 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [02:24<00:00,  8.25it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 2\tBatch: [1000/1000]\tLoss: 1.2513\tAccuracyBase: 65.28 % (66.80 %)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 1000/1000 [02:24<00:00,  6.93it/s]\n",
            "100%|| 2000/2000 [02:45<00:00, 12.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best model saving!!!\n",
            "Val_Epoch: [2/2]\tAccuracyBoth: 35.93 +- 0.21 %\tAccuracyBase: 54.01 +- 0.25 %\tAccuracyNovel: 48.85 +- 0.42 %\n",
            "Elapsed Time: 21m/21m\n",
            "\n",
            "Best model saving!!!\tBest_Epoch: [2/2]\tAccuracyNovel: 48.85 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSkUw4I3n_i_"
      },
      "source": [
        "#### Test Only Vision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRAAtvwOn5T6"
      },
      "source": [
        "parser = argparse.ArgumentParser()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6Jyb4vCn5w3",
        "outputId": "d4e433ad-db15-46ca-c845-a317e7752ca5"
      },
      "source": [
        "#********************* Model/Dataset/Path Config *********************#\r\n",
        "parser.add_argument('--save_path', default='experiments')\r\n",
        "parser.add_argument('--network', type=str, default='Conv64',\r\n",
        "                    help='choose which embedding network to use')\r\n",
        "parser.add_argument('--dataset', type=str, default='miniImageNet',\r\n",
        "                    help='choose which classification head to use. miniImageNet, tieredImageNet')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--dataset'], dest='dataset', nargs=None, const=None, default='miniImageNet', type=<class 'str'>, choices=None, help='choose which classification head to use. miniImageNet, tieredImageNet', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4aLy5EPn6AS",
        "outputId": "330bcb24-27cc-44c4-b896-8dff7c42414a"
      },
      "source": [
        "#********************* Testing Config *********************#\r\n",
        "parser.add_argument('--test_nKnovel', type=int, default=5, help='number of novel categories during 1-stage_base testing phase')\r\n",
        "parser.add_argument('--test_nKbase', type=int, default=64, help='number of base categories during 1-stage_base testing phase')\r\n",
        "parser.add_argument('--test_nExemplars', type=int, default=5, help='number of support examples per novel category')\r\n",
        "parser.add_argument('--test_nTestNovel', type=int, default=15*5, help='number of query examples for all the novel category')\r\n",
        "parser.add_argument('--test_nTestBase', type=int, default=15*5, help='number of test examples for all the base category')\r\n",
        "parser.add_argument('--test_batch_size', type=int, default=1, help='number of episodes per batch')\r\n",
        "parser.add_argument('--test_epoch_size', type=int, default=600, help='number of batchs per epoch')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--test_epoch_size'], dest='test_epoch_size', nargs=None, const=None, default=600, type=<class 'int'>, choices=None, help='number of batchs per epoch', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODP4ANwaonp8",
        "outputId": "dade2375-0321-4460-d30c-a8dcebbbb4fb"
      },
      "source": [
        "params = parser.parse_args(\"\")\r\n",
        "all_defaults = {}\r\n",
        "for key in vars(params):\r\n",
        "    all_defaults[key] = parser.get_default(key)\r\n",
        "all_defaults"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dataset': 'miniImageNet',\n",
              " 'network': 'Conv64',\n",
              " 'save_path': 'experiments',\n",
              " 'test_batch_size': 1,\n",
              " 'test_epoch_size': 600,\n",
              " 'test_nExemplars': 5,\n",
              " 'test_nKbase': 64,\n",
              " 'test_nKnovel': 5,\n",
              " 'test_nTestBase': 75,\n",
              " 'test_nTestNovel': 75}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5VabgjnooAz",
        "outputId": "1cafe05b-35af-41c9-d6a9-0083a16289b4"
      },
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n",
        "print('using gpu:', '0')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using gpu: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJHigadBooP_"
      },
      "source": [
        "#save_path = os.path.join(params.save_path, params.dataset, params.network)\r\n",
        "#mkdir((save_path))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2mB1-Qwn6M-",
        "outputId": "3517535c-1710-4f50-f0df-293719d9a235"
      },
      "source": [
        "dataset_test = MiniImageNet(phase='test')\r\n",
        "dataloder_test = FewShotDataloader(\r\n",
        "    dataset=dataset_test,\r\n",
        "    nKnovel=params.test_nKnovel,\r\n",
        "    nKbase=params.test_nKbase,\r\n",
        "    nExemplars=params.test_nExemplars,\r\n",
        "    nTestNovel=params.test_nTestNovel,\r\n",
        "    nTestBase=params.test_nTestBase,\r\n",
        "    batch_size=params.test_batch_size,\r\n",
        "    num_workers= 4,\r\n",
        "    epoch_size=params.test_epoch_size,\r\n",
        ")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading mini ImageNet dataset - phase test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0ife4T1pIw4"
      },
      "source": [
        "best_model = []\r\n",
        "for par in range(2):\r\n",
        "#     if params.network[par] == 'Conv64':\r\n",
        "#         embedding_model = Conv64().cuda()\r\n",
        "#         classifier = KTN_Classifier(nKall=64, nFeat=64 * 5 * 5, scale_cls=10).cuda()\r\n",
        "#     elif params.network[par] == 'Conv128':\r\n",
        "#         embedding_model = Conv128().cuda()\r\n",
        "#         classifier = KTN_Classifier(nKall=64, nFeat=128 * 5 * 5, scale_cls=10).cuda()\r\n",
        "#     # else:  # the other backbones are coming soon!\r\n",
        "#     #     raise ValueError('Unknown model')\r\n",
        "    \r\n",
        "    best = torch.load(os.path.join(dir[par],'1_stage_best_model.pth'))\r\n",
        "    best_model.append(best)\r\n",
        "    embedding_model.load_state_dict(best_model[par]['embedding'])\r\n",
        "    classifier.load_state_dict(best_model[par]['classifier'])\r\n",
        "    embedding_model.load_state_dict(best_model[par]['embedding'])\r\n",
        "    classifier.load_state_dict(best_model[par]['classifier'])\r\n",
        "    _, _ = [x.eval() for x in (embedding_model, classifier)]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A2WTExFpmn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "650235ca-1104-4f62-e4e5-e5a243ec6b1c"
      },
      "source": [
        "test_accuracies_both = []\r\n",
        "test_accuracies_base = []\r\n",
        "test_accuracies_novel = []\r\n",
        "\r\n",
        "for i, batch in enumerate(tqdm(dataloder_test()), 1):\r\n",
        "\r\n",
        "    data_support, labels_support, data_query, labels_query, all_Kids, nKbase = [x.cuda() for x in batch]\r\n",
        "\r\n",
        "    labels_support_one_hot = one_hot(labels_support.reshape(-1) - 64, 5).unsqueeze(dim=0)\r\n",
        "\r\n",
        "    Kbase_ids = Variable(all_Kids[:, :nKbase].contiguous(), requires_grad=False)\r\n",
        "    Knovel_ids = Variable(all_Kids[:, nKbase:].contiguous(), requires_grad=False)\r\n",
        "\r\n",
        "    emb_data_support = embedding_model(data_support.view([-1] + list(data_support.shape[-3:])))\r\n",
        "    emb_data_support = emb_data_support.view(params.test_batch_size, params.test_nExemplars*params.test_nKnovel, -1)\r\n",
        "\r\n",
        "    emb_data_query = embedding_model(data_query.view([-1] + list(data_query.shape[-3:])))\r\n",
        "    emb_data_query = emb_data_query.view(params.test_batch_size, params.test_nTestBase + params.test_nTestNovel,-1)\r\n",
        "\r\n",
        "    cls_scores = classifier(features_query=emb_data_query, Kbase_ids=Kbase_ids,Knovel_ids = Knovel_ids,\r\n",
        "                            features_support=emb_data_support, labels_support=labels_support_one_hot).view(\r\n",
        "        params.test_batch_size * (params.test_nTestBase + params.test_nTestNovel), -1)\r\n",
        "\r\n",
        "    accuracyBoth, accuracyBase, accuracyNovel = top1accuracy_all(cls_scores, labels_query.view(-1), nKbase)\r\n",
        "\r\n",
        "    test_accuracies_both.append(accuracyBoth.item())\r\n",
        "    test_accuracies_base.append(accuracyBase.item())\r\n",
        "    test_accuracies_novel.append(accuracyNovel.item())"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 600/600 [00:53<00:00, 11.18it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87c6l7uIpJAI"
      },
      "source": [
        "test_acc_both = np.mean(np.array(test_accuracies_both))\r\n",
        "test_acc_both_ci95 = 1.96 * np.std(np.array(test_accuracies_both)) / np.sqrt(params.test_epoch_size)\r\n",
        "\r\n",
        "test_acc_base = np.mean(np.array(test_accuracies_base))\r\n",
        "test_acc_base_ci95 = 1.96 * np.std(np.array(test_accuracies_base)) / np.sqrt(params.test_epoch_size)\r\n",
        "\r\n",
        "test_acc_novel = np.mean(np.array(test_accuracies_novel))\r\n",
        "test_acc_novel_ci95 = 1.96 * np.std(np.array(test_accuracies_novel)) / np.sqrt(params.test_epoch_size)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg7MfzcTpJRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e6a349-0f80-497a-9c0b-65b33971e97c"
      },
      "source": [
        "print('AccuracyBoth: {:.2f} +- {:.2f} %\\tAccuracyBase: {:.2f} +- {:.2f} %\\tAccuracyNovel: {:.2f} +- {:.2f} %'.format(\r\n",
        "            test_acc_both, test_acc_both_ci95, test_acc_base, test_acc_base_ci95,test_acc_novel, test_acc_novel_ci95))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AccuracyBoth: 47.10 +- 0.40 %\tAccuracyBase: 53.55 +- 0.47 %\tAccuracyNovel: 65.78 +- 0.68 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIRU8t8g8rY-"
      },
      "source": [
        "#### Testing Visual Knowledge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLCJbiTUlAEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c04e2189-e941-48ce-bb12-d9b1128ad2ee"
      },
      "source": [
        "parser = argparse.ArgumentParser()\r\n",
        "\r\n",
        "parser.add_argument('--num_epoch', type=int, default=1,\r\n",
        "                    help='number of training epochs')\r\n",
        "parser.add_argument('--save_path', default='experiments')\r\n",
        "parser.add_argument('--network', type=str, default='Conv64',\r\n",
        "                    help='choose which embedding network to use')\r\n",
        "parser.add_argument('--dataset', type=str, default='miniImageNet',\r\n",
        "                    help='choose which classification head to use. miniImageNet, tieredImageNet')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--dataset'], dest='dataset', nargs=None, const=None, default='miniImageNet', type=<class 'str'>, choices=None, help='choose which classification head to use. miniImageNet, tieredImageNet', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1nm7CUglF7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a214d5cf-0456-4b76-e575-ed13a6f83bf5"
      },
      "source": [
        "#***********************************************************************************************\r\n",
        "parser.add_argument('--test_nKnovel', type=int, default=5, help='number of novel categories during 1-stage_base testing phase')\r\n",
        "parser.add_argument('--test_nKbase', type=int, default=64, help='number of base categories during 1-stage_base testing phase')\r\n",
        "parser.add_argument('--test_nExemplars', type=int, default=1, help='number of support examples per novel category')\r\n",
        "parser.add_argument('--test_nTestNovel', type=int, default=15*5, help='number of query examples for all the novel category')\r\n",
        "parser.add_argument('--test_nTestBase', type=int, default=15*5, help='number of test examples for all the base category')\r\n",
        "parser.add_argument('--test_batch_size', type=int, default=1, help='number of episodes per batch')\r\n",
        "parser.add_argument('--test_epoch_size', type=int, default=600, help='number of batchs per epoch')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--test_epoch_size'], dest='test_epoch_size', nargs=None, const=None, default=600, type=<class 'int'>, choices=None, help='number of batchs per epoch', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyo_UZxClMWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d8568c-15ae-4f27-8d23-ad64afb7a87a"
      },
      "source": [
        "params = parser.parse_args(\"\")\r\n",
        "all_defaults = {}\r\n",
        "for key in vars(params):\r\n",
        "    all_defaults[key] = parser.get_default(key)\r\n",
        "all_defaults"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dataset': 'miniImageNet',\n",
              " 'network': 'Conv64',\n",
              " 'num_epoch': 1,\n",
              " 'save_path': 'experiments',\n",
              " 'test_batch_size': 1,\n",
              " 'test_epoch_size': 600,\n",
              " 'test_nExemplars': 1,\n",
              " 'test_nKbase': 64,\n",
              " 'test_nKnovel': 5,\n",
              " 'test_nTestBase': 75,\n",
              " 'test_nTestNovel': 75}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ycohZonlR5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5ad419-3258-4c69-c7e3-9691654f1708"
      },
      "source": [
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\n",
        "print('using gpu:', '0')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using gpu: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bD6knZmlW58"
      },
      "source": [
        "#save_path = os.path.join(params.save_path, params.dataset, params.network)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2wdMPyAlnGA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "a42d4b24-7c28-4474-89cb-55e54c166d9a"
      },
      "source": [
        "# Only for Conv64 Model\r\n",
        "pred_fcw = torch.load('/Conv64_fc.pred')\r\n",
        "\r\n",
        "dataset_test = MiniImageNet(phase='test')\r\n",
        "dataloder_test = FewShotDataloader(\r\n",
        "    dataset=dataset_test,\r\n",
        "    nKnovel=params.test_nKnovel,\r\n",
        "    nKbase=params.test_nKbase,\r\n",
        "    nExemplars=params.test_nExemplars,\r\n",
        "    nTestNovel=params.test_nTestNovel,\r\n",
        "    nTestBase=params.test_nTestBase,\r\n",
        "    batch_size=params.test_batch_size,\r\n",
        "    num_workers= 4,\r\n",
        "    epoch_size=params.test_epoch_size,\r\n",
        ")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-ff9bd069800c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Only for Conv64 Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred_fcw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Conv64_fc.pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniImageNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m dataloder_test = FewShotDataloader(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Conv64_fc.pred'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA064DnLlq_b"
      },
      "source": [
        "if params.network == 'Conv64':\r\n",
        "    embedding_model = Conv64().cuda()\r\n",
        "    classifier = KTN_Classifier(nKall=64, nFeat=64 * 5 * 5, scale_cls=10).cuda()\r\n",
        "else:  # the other backbones are coming soon!\r\n",
        "    raise ValueError('Unknown models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upDAiJXgluew"
      },
      "source": [
        "best_model = torch.load(os.path.join(save_path,'1_stage_best_model.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu7USuPFlyLJ"
      },
      "source": [
        "embedding_model.load_state_dict(best_model['embedding'])\r\n",
        "classifier.load_state_dict(best_model['classifier'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yySe5L26l2zW"
      },
      "source": [
        "_, _ = [x.eval() for x in (embedding_model, classifier)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgoEMb8wl9p7"
      },
      "source": [
        "test_accuracies_both = []\r\n",
        "test_accuracies_base = []\r\n",
        "test_accuracies_novel = []\r\n",
        "\r\n",
        "for i, batch in enumerate(tqdm(dataloder_test()), 1):\r\n",
        "\r\n",
        "    data_support, labels_support, data_query, labels_query, all_Kids, nKbase = [x.cuda() for x in batch]\r\n",
        "\r\n",
        "    labels_support_one_hot = one_hot(labels_support.reshape(-1) - 64, 5).unsqueeze(dim=0)\r\n",
        "\r\n",
        "    Kbase_ids = Variable(all_Kids[:, :nKbase].contiguous(), requires_grad=False)\r\n",
        "    Knovel_ids = Variable(all_Kids[:, nKbase:].contiguous(), requires_grad=False)\r\n",
        "\r\n",
        "    emb_data_support = embedding_model(data_support.view([-1] + list(data_support.shape[-3:])))\r\n",
        "    emb_data_support = emb_data_support.view(params.test_batch_size, params.test_nExemplars*params.test_nKnovel, -1)\r\n",
        "\r\n",
        "    emb_data_query = embedding_model(data_query.view([-1] + list(data_query.shape[-3:])))\r\n",
        "    emb_data_query = emb_data_query.view(params.test_batch_size, params.test_nTestBase + params.test_nTestNovel,-1)\r\n",
        "\r\n",
        "    cls_scores = classifier(features_query=emb_data_query, Kbase_ids=Kbase_ids,\r\n",
        "                            Knovel_ids = Knovel_ids,pred_fcw = pred_fcw,\r\n",
        "                            features_support=emb_data_support, labels_support=labels_support_one_hot).view(\r\n",
        "        params.test_batch_size * (params.test_nTestBase + params.test_nTestNovel), -1)\r\n",
        "\r\n",
        "    accuracyBoth, accuracyBase, accuracyNovel = top1accuracy_all(cls_scores, labels_query.view(-1), nKbase)\r\n",
        "\r\n",
        "    test_accuracies_both.append(accuracyBoth.item())\r\n",
        "    test_accuracies_base.append(accuracyBase.item())\r\n",
        "    test_accuracies_novel.append(accuracyNovel.item())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc-HqLCP8rY_"
      },
      "source": [
        "test_acc_both = np.mean(np.array(test_accuracies_both))\n",
        "test_acc_both_ci95 = 1.96 * np.std(np.array(test_accuracies_both)) / np.sqrt(params.test_epoch_size)\n",
        "\n",
        "test_acc_base = np.mean(np.array(test_accuracies_base))\n",
        "test_acc_base_ci95 = 1.96 * np.std(np.array(test_accuracies_base)) / np.sqrt(params.test_epoch_size)\n",
        "\n",
        "test_acc_novel = np.mean(np.array(test_accuracies_novel))\n",
        "test_acc_novel_ci95 = 1.96 * np.std(np.array(test_accuracies_novel)) / np.sqrt(params.test_epoch_size)\n",
        "\n",
        "print('AccuracyBoth: {:.2f} +- {:.2f} %\\tAccuracyBase: {:.2f} +- {:.2f} %\\tAccuracyNovel: {:.2f} +- {:.2f} %'.format(\n",
        "            test_acc_both, test_acc_both_ci95, test_acc_base, test_acc_base_ci95,test_acc_novel, test_acc_novel_ci95))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XZgBpsjk9_w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbyoTQyj8rY_"
      },
      "source": [
        "# Test  only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUPEP_bj8rZA"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}